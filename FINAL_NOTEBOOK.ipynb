{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "We load the data using a regular expression and a predetermined path. The data itself was downloaded from data.police.uk. Using regular expressions in `BASH` they were extracted into a seperated folder, for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = `<PATH OF DATA HERE>``\n",
    "all_files = glob.glob(path + \"/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_files_to_df(lstFiles):\n",
    "    '''\n",
    "    takes a lst of filenames, will transform these files into one dataframe\n",
    "    note: it will only work with csv files with same columns, or pandas will break\n",
    "    '''\n",
    "    \n",
    "    lst = []\n",
    "    \n",
    "    for filename in lstFiles:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        lst.append(df)\n",
    "        \n",
    "    dataframe = pd.concat(lst, axis=0, ignore_index=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = concat_files_to_df(all_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_start = all_data.copy()\n",
    "N = len(df_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_start['Crime ID'].unique()) / N\n",
    "\n",
    "# conclusion ~33% of crimes have no ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the Month column, with integer values\n",
    "df_start['year'] = pd.DatetimeIndex(df_start['Month']).year\n",
    "df_start['month'] = pd.DatetimeIndex(df_start['Month']).month\n",
    "df_start.drop('Month', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop these two columns as they are useless\n",
    "df_start.drop(['Reported by', 'Falls within'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have multiple variables that give location data, location as a column is non specific and needs to be encoded on top to be worked with\n",
    "# therefore we can drop the column\n",
    "df_start.drop('Location', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only one LSOA identifier is enough\n",
    "df_start.drop('LSOA code', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one third of the dataset does not have an outcome category, as we are currently NOT specifying crime type that importantly\n",
    "# we can drop the column now, but if we think crime type is very important, we will have to add it back in, for more nuance\n",
    "# we would also have to further look into the nan values then\n",
    "df_start[df_start['Last outcome category'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_start.drop('Last outcome category', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysing the `Context` column, it can be concluded that some crimes have been allocated the nearest possible location, as the true location of the crime could not be properly mapped. Therefore, as the location is not precise, we consider this data to be faulty and we will delete these data entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep non context values\n",
    "df_start = df_start[df_start['Context'].isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can drop the Context column, as it doesn't hold anymore information\n",
    "df_start.drop('Context', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after some consideration, we decided to drop the crime ID column for now, it can easily be put back in if we want to change our approach\n",
    "df_start.drop('Crime ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding extra features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with adding the median incomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_police = df_start.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a set of all of the areas not in the Greater Manchester Area\n",
    "\n",
    "df_police['Borough'] = df_police['LSOA name'].str[:-5] # Add first part of LSOA name as borough in the dataframe\n",
    "\n",
    "lst_boroughs = [\"Manchester\", \"Salford\", \"Bolton\", \"Bury\", \"Oldham\", \"Rochdale\", \"Stockport\", \n",
    "                \"Tameside\", \"Trafford\", \"Wigan\"] # List contains all the boroughs of the Greater Manchester Area\n",
    "\n",
    "lst_boroughs_in_df = df_police['Borough'].unique() # List contains all unique boroughs in the df_police dataframe\n",
    "\n",
    "set_incorrect_boroughs = set(lst_boroughs_in_df) - set(lst_boroughs) # Set contains all areas that are in the dataframe but are not in the Greater Manchester Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove areas that don't fall within the Greater Manchester Area\n",
    "\n",
    "for borough in set_incorrect_boroughs:\n",
    "    df_police = df_police[df_police['Borough'] != borough]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Median Annual Gross Pay data\n",
    "\n",
    "df_pay = pd.read_excel('AnnualPayGrossManchester.xlsx')\n",
    "df_pay = df_pay.set_index('Region')\n",
    "df_pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique years in the df_police\n",
    "\n",
    "df_police = df_police.dropna() # Drop row with a NaN value for Borough\n",
    "lst_years = df_police['year'].unique() # List contains all unique years in df_police dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Median Annual Gross Pay to rows dependent of borough and year\n",
    "\n",
    "df = pd.DataFrame() # Create empty dataframe to add agp to all combinations of boroughs and years seperately\n",
    "\n",
    "for borough in lst_boroughs:\n",
    "    df_borough = df_police.copy()\n",
    "    df_borough = df_borough[df_borough['Borough'] == borough]\n",
    "    for year in lst_years:\n",
    "        df_year = df_borough.copy()\n",
    "        df_year = df_year[df_year['year'] == year]\n",
    "        df_year['magp'] = dict(df_pay)[year]['  ' + borough]\n",
    "        df = pd.concat([df, df_year])\n",
    "\n",
    "\n",
    "df_police = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the average age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_age_per_year(file_name):\n",
    "    \n",
    "    df = pd.read_excel(file_name)\n",
    "    \n",
    "    lst = []\n",
    "    lst.append(\"LSOA Name\")\n",
    "    for i in range(90):\n",
    "        lst.append(i)\n",
    "    lst.append(\"90+\")\n",
    "    \n",
    "    df = df[lst]\n",
    "    \n",
    "    df['Borough'] = df['LSOA Name'].str[:-5]\n",
    "    lst = [\"Manchester\", \"Salford\", \"Bolton\", \"Bury\", \"Oldham\", \"Rochdale\", \"Stockport\", \"Tameside\", \"Trafford\", \"Wigan\"]\n",
    "    lst2 = df['Borough'].unique()\n",
    "    s1 = set(lst2) - set(lst)\n",
    "    \n",
    "    for i in s1:\n",
    "        df = df[df['Borough'] != i]\n",
    "    \n",
    "    df = df.dropna()\n",
    "    \n",
    "    df = df.set_index('LSOA Name')\n",
    "    df = df.drop(columns=[\"Borough\"])\n",
    "    df = df.rename(columns={'90+': 90})\n",
    "    \n",
    "    dic = dict()\n",
    "    for lsoa in df.index.unique():\n",
    "        dic[lsoa] = (df.T[lsoa] * df.T.index).sum() / df.T[lsoa].sum()\n",
    "        \n",
    "    df['avg'] = pd.Series(dic)\n",
    "    df = df['avg']\n",
    "    output = dict(df)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_avg_age_per_year(files):\n",
    "    \n",
    "    lst_years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "    df_return = pd.DataFrame()\n",
    "    \n",
    "    for year, file in zip(lst_years, files):\n",
    "        df = df_police[df_police['year'] == year]\n",
    "        df['avg age'] = df['LSOA name'].map(get_avg_age_per_year(file))\n",
    "        df_return = pd.concat([df_return, df])\n",
    "        \n",
    "    return df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_files = ['age_2012.xlsx', 'age_2012.xlsx', 'age_2013.xlsx', 'age_2014.xlsx', 'age_2015.xlsx', 'age_2016.xlsx',\n",
    "             'age_2017.xlsx', 'age_2018.xlsx', 'age_2019.xlsx', 'age_2020.xlsx']\n",
    "df = add_avg_age_per_year(lst_files)\n",
    "# File for 2011 uses age groups and can therefore not be used to calculate an accurate average\n",
    "# I suggest using the 2012 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_data = pd.read_pickle(\"crime.pickle\")\n",
    "raw_data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {\"Longitude\": \"longitude\", \"Latitude\": \"latitude\", \"LSOA name\": \"LSOA\", \"Crime type\": \"crime_type\", \n",
    "           \"magp\": \"median_income\", \"avg age\": \"avg_age\", \"Borough\": \"borough\", \"Cluster\": \"cluster\", \"Season\": \"season\"}\n",
    "raw_data.rename(columns=dct, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_data(df):\n",
    "    \"\"\"\n",
    "    Extracts the different seasons in the data\n",
    "    \"\"\"\n",
    "    spring = df[(df['month'] == 3) | (df['month'] == 4) | (df['month'] == 5)]\n",
    "    summer = df[(df['month'] == 6) | (df['month'] == 7) | (df['month'] == 8)]\n",
    "    fall = df[(df['month'] == 9) | (df['month'] == 10) | (df['month'] == 11)]\n",
    "    winter = df[(df['month'] == 12) | (df['month'] == 1) | (df['month'] == 2)]\n",
    "    return spring, summer, fall, winter\n",
    "\n",
    "def locational_data(df, x='longitude', y='latitude'):\n",
    "    \"\"\"\n",
    "    Extracs just location data\n",
    "    \"\"\"\n",
    "    return df[[x, y]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make a copy before proceeding\n",
    "data = raw_data[(raw_data[\"year\"] != 2019) & (raw_data[\"year\"] != 2010)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering = data.sort_values([\"month\", \"year\"], ascending=(True, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split off the seasons, this was done to check how hotspots behaved here, this is not part of the main script, but it's shown here for reference how this was obtained\n",
    "spring, summer, fall, winter = seasonal_data(df_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spring_loc = locational_data(spring)\n",
    "summer_loc = locational_data(summer)\n",
    "fall_loc = locational_data(fall)\n",
    "winter_loc = locational_data(winter)\n",
    "cluster_loc = locational_data(df_clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue from here on using the cluster_loc data, the seasonal data is invalid for the final project. We sample the data to severly speed up the KMeans Constrained algorithm. As we are random sampling this much data, the clusters are still representative. To extrapolate them we will use a `Gradient Booster`, this had a 99.6% accuracry in assigning the clusters. This is lots faster than training the KMeans Constrained for the added date, on top of the many, many, many memory errors we ran into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(df, x=\"Longitude\", y=\"Latitude\", frc=0.1):\n",
    "    \"\"\"\n",
    "    samples data\n",
    "    \"\"\"\n",
    "    sample_loc = df.sample(frac=frc, random_state=42)\n",
    "    return sample_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_loc = sampler(cluster_loc, frc=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important background information, to find this main hotspot, so called data shader plots were used. However, the implementation of them in this main script is difficult. They use a library depency called `Numba`, this library has a conflicting `numpy` version dependency with `KMeans Constrained`. As the latter is more important for our overal performance this was ommited from the final result. The code to get a plot like this is added, but it is commented out so it doesn't crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datashader as ds\n",
    "# from datashader.mpl_ext import dsshow\n",
    "\n",
    "\n",
    "# def using_datashader(ax, x, y, mx=80):\n",
    "#     df = pd.DataFrame(dict(x=x, y=y))\n",
    "#     dsartist = dsshow(\n",
    "#         df,\n",
    "#         ds.Point(\"x\", \"y\"),\n",
    "#         ds.count(),\n",
    "#         vmin=0,\n",
    "#         vmax=mx,\n",
    "#         norm=\"linear\",\n",
    "#         aspect=\"auto\",\n",
    "#         ax=ax,\n",
    "#     )\n",
    "\n",
    "#     plt.colorbar(dsartist)\n",
    "\n",
    "\n",
    "# # fig, ax = plt.subplots()\n",
    "# # using_datashader(ax, loc_x, loc_y)\n",
    "# # fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "# # ax.annotate('axes fraction',\n",
    "# #             xy=(-2.22, 53.48),\n",
    "# #             xytext=(-2.22, 53.48), textcoords='X')\n",
    "\n",
    "# # # Clusters:\n",
    "# # # (-2.18, 53.63)\n",
    "# # # (-2.13, 53.54)\n",
    "# # # (-2.1, 53.48) - klein\n",
    "# # # (-2.28, 53.58)\n",
    "# # # (-2.42, 53.57)\n",
    "\n",
    "# # plt.text(-2.2425, 53.48, \"X\", size=16, weight='bold')\n",
    "# # plt.text(-2.167, 53.615, \"X\", size=14)\n",
    "# # plt.text(-2.123, 53.54, \"X\", size=14)\n",
    "# # plt.text(-2.3, 53.59, \"X\", size=14)\n",
    "# # plt.text(-2.438, 53.576, \"X\", size=14)\n",
    "# # plt.text(-2.1, 53.49, \"X\", size=12)\n",
    "\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_hotspot_grid(df, x1=-2.22, x2=-2.26, y1=53.46, y2=53.49):\n",
    "    \"\"\"\n",
    "    Returns the data regarding the main hotspot cluster in the GMA\n",
    "    \"\"\"\n",
    "    return df[(df['latitude'] > y1) & (df['latitude'] < y2) & (df['longitude'] > x2) & (df['longitude'] < x1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following holds for the data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'main_hotspot_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 47'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000048untitled?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mThe following holds for the data\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000048untitled?line=1'>2</a>\u001b[0m hotspot \u001b[39m=\u001b[39m main_hotspot_grid(sample_loc)\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000048untitled?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThere are roughly \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(hotspot)\u001b[39m}\u001b[39;00m\u001b[39m data points in the main hotspot, suggesting an esmitation for the max cluster size\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000048untitled?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThere is a total of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(sample_loc)\u001b[39m}\u001b[39;00m\u001b[39m data points\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'main_hotspot_grid' is not defined"
     ]
    }
   ],
   "source": [
    "print('The following holds for the data')\n",
    "hotspot = main_hotspot_grid(sample_loc)\n",
    "print(f'There are roughly {len(hotspot)} data points in the main hotspot, suggesting an esmitation for the max cluster size')\n",
    "print(f'There is a total of {len(sample_loc)} data points')\n",
    "print(f'The minimum number of clusters from this simple calculation would be: {math.ceil(len(sample_loc)/len(hotspot))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_array = np.array(sample_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from k_means_constrained import KMeansConstrained\n",
    "\n",
    "clf_clusters = KMeansConstrained(\n",
    "     n_clusters=13,\n",
    "     size_min=None,\n",
    "     size_max=22000,\n",
    "     random_state=42,\n",
    "     n_jobs=-1\n",
    ")\n",
    "clf_clusters.fit_predict(clusters_array)\n",
    "\n",
    "sample_labels = clf_clusters.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the clusters to the appropriate labels\n",
    "sample_loc['cluster'] = pd.Series(sample_labels.copy(), index=sample_loc.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_x = sample_loc.drop(\"cluster\", axis=1)\n",
    "data_y = sample_loc[\"cluster\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbrt = GradientBoostingClassifier()\n",
    "gbrt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbrt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "prec_score = precision_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "rec_score = recall_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "F1_score = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'The accuracy of the model is {round(acc_score, 4)}.')\n",
    "print(f'The precision of the model is {round(prec_score, 4)}, using weighted average.')\n",
    "print(f'The recall of the model is {round(rec_score, 4)}, using weighted average.')\n",
    "print(f'The f1-score of the model is {round(F1_score, 4)} using weighted average.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_test = locational_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrapolating the clusters\n",
    "test_predict = gbrt.predict(loc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the clusters to the data\n",
    "data['cluster'] = pd.Series(test_predict.copy(), index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_season(month, hemisphere=\"Northern\"):\n",
    "    \"\"\"\n",
    "    Adding seasons to the data set, depending on the hemisphere\n",
    "    \"\"\"\n",
    "    if hemisphere == 'Southern':\n",
    "        season_month_south = {\n",
    "            12:'Summer', 1:'Summer', 2:'Summer',\n",
    "            3:'Autumn', 4:'Autumn', 5:'Autumn',\n",
    "            6:'Winter', 7:'Winter', 8:'Winter',\n",
    "            9:'Spring', 10:'Spring', 11:'Spring'}\n",
    "        return season_month_south.get(month)\n",
    "        \n",
    "    elif hemisphere == 'Northern':\n",
    "        season_month_north = {\n",
    "            12:'Winter', 1:'Winter', 2:'Winter',\n",
    "            3:'Spring', 4:'Spring', 5:'Spring',\n",
    "            6:'Summer', 7:'Summer', 8:'Summer',\n",
    "            9:'Autumn', 10:'Autumn', 11:'Autumn'}\n",
    "        return season_month_north.get(month)\n",
    "    else:\n",
    "        print('Invalid selection. Please select a hemisphere and try again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_list = []\n",
    "hemisphere = 'Northern'\n",
    "for month in data.month:\n",
    "    season = find_season(month, hemisphere)\n",
    "    season_list.append(season)\n",
    "    \n",
    "data['season'] = season_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models & Adding some Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {\"Longitude\": \"longitude\", \"Latitude\": \"latitude\", \"LSOA name\": \"LSOA\", \"Crime type\": \"crime_type\", \n",
    "           \"magp\": \"median_income\", \"avg age\": \"avg_age\", \"Borough\": \"borough\", \"Cluster\": \"cluster\", \"Season\": \"season\"}\n",
    "data.rename(columns=dct, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change cluster 0 to 13, just looks nicer\n",
    "data.loc[data.cluster==0, \"cluster\"] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding previous month to the data for an easier identifier \n",
    "data[\"prev_month\"] = np.nan\n",
    "\n",
    "lst_months = sorted(data.month.unique().tolist())\n",
    "lst_prev = [12] + lst_months[:11]\n",
    "\n",
    "\n",
    "i = 0\n",
    "while i < 12:\n",
    "    data.loc[data.month == lst_months [i], \"prev_month\"] = lst_prev[i]\n",
    "    i += 1\n",
    "\n",
    "data.prev_month = data.prev_month.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding different types of crime statistics, currently the only one that is used is total monthly crime `monthly`\n",
    "# this however easily could be build upon for different models/time series analyses given more time\n",
    "\n",
    "# year\n",
    "data[\"yearly\"] = data.groupby([\"year\", \"cluster\"])[\"crime_type\"].transform(\"count\")\n",
    "data[\"yearly_crime\"] = data.groupby([\"year\", \"crime_type\", \"cluster\"])[\"crime_type\"].transform(\"count\")\n",
    "\n",
    "# month\n",
    "data[\"monthly\"] = data.groupby([\"year\", \"month\", \"cluster\"])[\"crime_type\"].transform(\"count\")\n",
    "data[\"monthly_crime\"] = data.groupby([\"year\", \"month\", \"crime_type\", \"cluster\"])[\"crime_type\"].transform(\"count\")\n",
    "\n",
    "# seasonality\n",
    "data[\"seasonality\"] = data.groupby([\"year\", \"season\", \"cluster\"])[\"crime_type\"].transform(\"count\")\n",
    "data[\"seasonality_crime\"] = data.groupby([\"year\", \"season\", \"crime_type\", \"cluster\"])[\"crime_type\"].transform(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to add a time feature that has a rolling window, therefore we need to do some data wrangling steps\n",
    "# due to the nature of the clustering, the data wrangling was a bit complicated and I did not find a way to do it\n",
    "# more elegant then using 3 for loops (technically calling the function every time for a new cluster is also a for loop)\n",
    "# however having the monthly data in seperate lists is usefull regardless to calculate the rolling mean more easily\n",
    "# reminder, there are 13! clusters in total\n",
    "\n",
    "\n",
    "lst_years = [2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018]\n",
    "clusters = sorted(data.cluster.unique().tolist())\n",
    "lst_months\n",
    "\n",
    "def rolling_mean(data, list_years=lst_years, list_months=lst_months, size=4):\n",
    "    \"\"\"\n",
    "    Outputs a list of the monthly crime per month sorted on time, this is used\n",
    "    to then calculate a rolling mean for this particular cluster\n",
    "    param: dataframe, this should only contain the cluster specific data\n",
    "    list of years, list of months, rolling window size\n",
    "    return: lst of the resulting rolling mean for this cluster\n",
    "    \"\"\"\n",
    "    lst = []\n",
    "\n",
    "    for i in list_years:\n",
    "        for j in list_months:   \n",
    "            lst.append(data.loc[(data.year==i) & (data.month==j)].monthly.unique()[0])\n",
    "    \n",
    "\n",
    "    windows = pd.Series(lst).rolling(size).mean()\n",
    "    return windows.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists that represent the rolling means, just a number behind lst is used to indicated the cluster\n",
    "# we can use previous months also in the test set as they will be known to the algorithm at the time of prediction\n",
    "\n",
    "lst1 = rolling_mean(data.loc[data.cluster==1])\n",
    "lst2 = rolling_mean(data.loc[data.cluster==2])\n",
    "lst3 = rolling_mean(data.loc[data.cluster==3])\n",
    "lst4 = rolling_mean(data.loc[data.cluster==4])\n",
    "lst5 = rolling_mean(data.loc[data.cluster==5])\n",
    "lst6 = rolling_mean(data.loc[data.cluster==6])\n",
    "lst7 = rolling_mean(data.loc[data.cluster==7])\n",
    "lst8 = rolling_mean(data.loc[data.cluster==8])\n",
    "lst9 = rolling_mean(data.loc[data.cluster==9])\n",
    "lst10 = rolling_mean(data.loc[data.cluster==10])\n",
    "lst11 = rolling_mean(data.loc[data.cluster==11])\n",
    "lst12 = rolling_mean(data.loc[data.cluster==12])\n",
    "lst13 = rolling_mean(data.loc[data.cluster==13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[[\"rolling_months\"]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_mean_adder(data, lst , Z, list_years=lst_years, list_months=lst_months):\n",
    "    \"\"\"\n",
    "    Adds the rolling mean values back into the dataframe\n",
    "    param: dataframe of all the data, lst containing the cluster rolling mean\n",
    "    list of years, list of months, rolling window size, Z an integer representing the cluster\n",
    "    return: None\n",
    "    \"\"\"\n",
    "    q = 0\n",
    "\n",
    "    while q < len(lst):\n",
    "        for i in list_years:\n",
    "            for j in list_months:\n",
    "                data.loc[(data.year==i) & (data.month==j) & (data.cluster==Z), \"rolling_months\"] = lst[q]\n",
    "                q += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst number corresponding to cluster number to add data to the clusters\n",
    "\n",
    "rolling_mean_adder(data, lst1, 1)\n",
    "rolling_mean_adder(data, lst1, 2)\n",
    "rolling_mean_adder(data, lst1, 3)\n",
    "rolling_mean_adder(data, lst1, 4)\n",
    "rolling_mean_adder(data, lst1, 5)\n",
    "rolling_mean_adder(data, lst1, 6)\n",
    "rolling_mean_adder(data, lst1, 7)\n",
    "rolling_mean_adder(data, lst1, 8)\n",
    "rolling_mean_adder(data, lst1, 9)\n",
    "rolling_mean_adder(data, lst1, 10)\n",
    "rolling_mean_adder(data, lst1, 11)\n",
    "rolling_mean_adder(data, lst1, 12)\n",
    "rolling_mean_adder(data, lst1, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add prev months crime rate this should be less effective then rolling window\n",
    "data[\"prev_month_crime\"] = np.nan\n",
    "\n",
    "j = 0\n",
    "while j < 12:\n",
    "    data.loc[data.month == lst_months[j], \"prev_month_crime\"] = data.loc[data.prev_month == lst_prev[j]].monthly.unique()[0]\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp was dropped in preprocessing, but for datatime ease we add it back in, all data was added on first day of month, so we can assing this\n",
    "# this was done to try and train an ARIMA model, this model gave RAM errors asking for 300GB+ of RAM\n",
    "data[\"day\"] = 1\n",
    "\n",
    "data[\"datetime\"] = pd.to_datetime(data[[\"year\", \"month\", \"day\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as data is timeseries we sort it one more time to be sure\n",
    "data.sort_values(\"datetime\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of the incomplete years as they can't be used properly currently\n",
    "data = data[~data.year.isin([2011, 2019])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test based off whole years\n",
    "train = data[~(data.year == 2018) & ~(data.year == 2017)]\n",
    "test = data[(data.year == 2018) | (data.year == 2017)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonality_pick(data, season,season_crime):\n",
    "    \"\"\"\n",
    "    dataframe gets transformed to represent the seasonality wanted\n",
    "    \"\"\"\n",
    "    return data[[\"crime_type\", \"datetime\", \"borough\", \"median_income\", \"avg_age\", \"cluster\", \"year\", \"season\", \"month\", \"rolling_months\", \"prev_month_crime\",season, season_crime]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the training sets \n",
    "train_month = seasonality_pick(train, \"monthly\", \"monthly_crime\")\n",
    "# train_year = seasonality_pick(train, \"yearly\", \"yearly_crime\")\n",
    "# train_season = seasonality_pick(train, \"seasonality\", \"seasonality_crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the test sets \n",
    "test_month = seasonality_pick(test, \"monthly\", \"monthly_crime\")\n",
    "# test_year = seasonality_pick(test, \"yearly\", \"yearly_crime\")\n",
    "# test_season = seasonality_pick(test, \"seasonality\", \"seasonality_crime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# not using the Pipeline for now, as we don't want to scale the rolling_months and I don't have time to figure out how to pass this\n",
    "# the Pipeline that is displayed is wrong and has to be build out with different Encoders regardless\n",
    "\n",
    "# num_pipeline = Pipeline([\n",
    "#     (\"std_scaler\", StandardScaler())\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoders\n",
    "ord_enc = OrdinalEncoder()\n",
    "borough_enc = LabelEncoder()\n",
    "crime_enc = LabelEncoder()\n",
    "inc_scl = StandardScaler()\n",
    "age_scl = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform encoding on the train set\n",
    "train_month.season = ord_enc.fit_transform(train_month[[\"season\"]])\n",
    "train_month.crime_type = crime_enc.fit_transform(train_month[\"crime_type\"])\n",
    "train_month.borough = borough_enc.fit_transform(train_month[\"borough\"])\n",
    "train_month.median_income = inc_scl.fit_transform(train_month[[\"median_income\"]])\n",
    "train_month.avg_age = age_scl.fit_transform(train_month[[\"avg_age\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the test set\n",
    "test_month.season = ord_enc.transform(test_month[[\"season\"]])\n",
    "test_month.crime_type = crime_enc.transform(test_month[\"crime_type\"])\n",
    "test_month.borough = borough_enc.transform(test_month[\"borough\"])\n",
    "test_month.median_income = inc_scl.transform(test_month[[\"median_income\"]])\n",
    "test_month.avg_age = age_scl.transform(test_month[[\"avg_age\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sets are intentionally commented out\n",
    "X_train = train_month[[\"crime_type\", \"borough\", \"median_income\", \"avg_age\", \"season\", \"month\", \"rolling_months\"]]\n",
    "# X_test = test_clst1[[\"crime_type\", \"borough\", \"median_income\", \"avg_age\", \"season\", \"month\", \"rolling_months\"]]\n",
    "y_train = train_month[\"monthly\"]\n",
    "# y_test = test_clst1[\"monthly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "def time_metrics(y_test, y_pred, model):\n",
    "    \"\"\"\n",
    "    print the time metrics\n",
    "    \"\"\"\n",
    "    print(f\"Error metrics for the {model} prediction model\")\n",
    "    print(\"\\n\")\n",
    "    print('Mean Absolute Error:', round(mean_absolute_error(y_test, y_pred), 3))\n",
    "    print('Mean Squared Error:', round(mean_squared_error(y_test, y_pred), 3))\n",
    "    print('Root Mean Squared Error:', round(np.sqrt(mean_squared_error(y_test, y_pred)), 3))\n",
    "    print('R2 score:', round(r2_score(y_test, y_pred), 3))\n",
    "\n",
    "def test_model(data, model, model_name):\n",
    "    \"\"\"\n",
    "    Tests the outcomes of the trained model per cluster\n",
    "    param: general test set, the variable storing the model function, model name as a string\n",
    "    return: prints of the error metrics\n",
    "    \"\"\"\n",
    "    for i in clusters:\n",
    "        test = data.loc[data.cluster==i]\n",
    "        X_test = test[[\"crime_type\", \"borough\", \"median_income\", \"avg_age\", \"season\", \"month\", \"rolling_months\"]]\n",
    "        y_test = test[\"monthly\"]\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(\"\\n\")\n",
    "        print(f\"Cluster {i}\")\n",
    "        time_metrics(y_test, y_pred, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression();\n",
    "lin_reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(test_month, lin_reg, \"Linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_reg = RandomForestRegressor()\n",
    "rdf_reg.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(test_month, rdf_reg, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb_reg = GradientBoostingRegressor()\n",
    "xgb_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(test_month, xgb_reg, \"Gradiant Boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# we ignore future warnings so the outcomes become more readable\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr_bst = xgb.XGBRFRegressor()\n",
    "xtr_bst.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(test_month, xtr_bst, \"Extreme Gradient Boosting\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47c473ffaad557a40d6f692199c8550b37e2966a5f36ac429a864ed95aaad2b0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
